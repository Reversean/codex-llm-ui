# Data Model: LLM Chat Interface

**Phase**: 1 (Design & Contracts)
**Date**: 2025-11-26
**Purpose**: Define data entities, their relationships, validation rules, and state transitions for the LLM chat interface.

## Overview

This document defines the core data structures used throughout the application. All entities are defined in TypeScript interfaces in the `shared/types.ts` file to ensure type safety across frontend and backend.

---

## Entity Diagram

```
┌─────────────────┐
│  Conversation   │
│    Session      │
└────────┬────────┘
         │
         │ 1:N
         ▼
   ┌───────────┐
   │  Message  │
   └─────┬─────┘
         │
         │ 1:1 (optional)
         ▼
  ┌──────────────┐
  │   Reasoning  │
  │    Block     │
  └──────────────┘
```

---

## Entity: Message

**Purpose**: Represents a single communication unit in the conversation (user input or LLM response).

### Fields

| Field | Type | Required | Description | Validation |
|-------|------|----------|-------------|------------|
| `id` | `string` | ✅ Yes | Unique identifier for the message | UUID v4 format |
| `content` | `string` | ✅ Yes | The message text content | Min 1 char, max 50,000 chars |
| `sender` | `'user' \| 'llm'` | ✅ Yes | Who sent the message | Must be 'user' or 'llm' |
| `timestamp` | `number` | ✅ Yes | Unix timestamp (ms) when message created | Must be positive integer |
| `status` | `MessageStatus` | ✅ Yes | Current message state | See MessageStatus enum |
| `reasoning` | `ReasoningBlock \| null` | ❌ No | LLM reasoning block (if available) | Only for LLM messages |

###  TypeScript Definition

```typescript
export type MessageStatus = 'pending' | 'streaming' | 'complete' | 'error';

export interface Message {
  id: string;
  content: string;
  sender: 'user' | 'llm';
  timestamp: number;
  status: MessageStatus;
  reasoning?: ReasoningBlock | null;
}
```

### State Transitions

```
User Message:
  pending → complete (instant, no streaming)

LLM Message:
  pending → streaming → complete
          ↓
        error (connection lost, LLM error)
```

### Business Rules

1. **ID Generation**: Frontend generates UUID v4 for user messages, backend generates for LLM messages
2. **Content Validation**:
   - User messages: Trim whitespace, reject empty strings (enforced by FR-015)
   - LLM messages: No validation (trusted source)
3. **Status Immutability**: Once `complete` or `error`, status cannot change
4. **Reasoning Availability**: Only LLM messages can have reasoning blocks

---

## Entity: Conversation Session

**Purpose**: Represents the complete dialog exchange between user and LLM for a single conversation.

### Fields

| Field | Type | Required | Description | Validation |
|-------|------|----------|-------------|------------|
| `messages` | `Message[]` | ✅ Yes | Ordered array of all messages | Array, ordered by timestamp |
| `state` | `SessionState` | ✅ Yes | Current conversation state | See SessionState enum |

### TypeScript Definition

```typescript
export type SessionState = 'idle' | 'thinking' | 'streaming' | 'error';

export interface ConversationSession {
  messages: Message[];
  state: SessionState;
}
```

### State Transitions

```
idle → thinking (user submits message)
     ↓
  streaming (LLM starts responding)
     ↓
  idle (LLM completes response)
     ↓
  error (connection lost, LLM error)
```

### Business Rules

1. **Single Session**: MVP supports one conversation session per user (no persistence, in-memory only)
2. **Message Ordering**: Messages MUST be appended in chronological order
3. **State Consistency**:
   - `thinking`: LLM is processing, no streaming yet
   - `streaming`: LLM response actively streaming
   - `idle`: Ready for next user message
   - `error`: User can retry or refresh
4. **Concurrency**: One active LLM response at a time (queue additional user messages)

---

## Entity: Reasoning Block

**Purpose**: Represents the LLM's "thinking" process output, displayed separately from the main response.

### Fields

| Field | Type | Required | Description | Validation |
|-------|------|----------|-------------|------------|
| `content` | `string` | ✅ Yes | The reasoning text generated by LLM | Min 1 char |
| `isExpanded` | `boolean` | ✅ Yes | Whether reasoning is currently visible | Default: true |

### TypeScript Definition

```typescript
export interface ReasoningBlock {
  content: string;
  isExpanded: boolean;
}
```

### Business Rules

1. **Default State**: Reasoning blocks start expanded (visible)
2. **User Interaction**: User can toggle expand/collapse via UI
3. **Rendering**: Displayed in muted/gray color above main LLM response
4. **Optional**: Not all LLM responses include reasoning

---

## Entity: Stream Chunk

**Purpose**: Represents a single chunk of data received during SSE streaming from the backend.

### Fields

| Field | Type | Required | Description | Validation |
|-------|------|----------|-------------|------------|
| `type` | `ChunkType` | ✅ Yes | Type of chunk | See ChunkType enum |
| `content` | `string` | ✅ Yes | The text content of this chunk | Can be empty for 'done' type |
| `messageId` | `string` | ✅ Yes | ID of message this chunk belongs to | UUID v4 format |

### TypeScript Definition

```typescript
export type ChunkType = 'chunk' | 'reasoning' | 'done' | 'error';

export interface StreamChunk {
  type: ChunkType;
  content: string;
  messageId: string;
}
```

### Chunk Types

- **`chunk`**: Regular content chunk for main LLM response
- **`reasoning`**: Reasoning text chunk (accumulated separately)
- **`done`**: Signals end of stream (content can be empty)
- **`error`**: Error occurred during streaming (content is error message)

### Business Rules

1. **Accumulation**: Frontend accumulates chunks for same `messageId`
2. **Ordering**: Chunks for one message arrive in sequence
3. **Reasoning Separation**: `reasoning` chunks accumulated separately from `chunk` types
4. **Completion**: `done` chunk marks stream completion, transition message to `complete` status

---

## Frontend State (Svelte Stores)

### Conversation Store

```typescript
// stores/conversation.ts
import { writable } from 'svelte/store';
import type { Message } from '$lib/types';

export const messages = writable<Message[]>([]);

export function addMessage(message: Message) {
  messages.update(msgs => [...msgs, message]);
}

export function updateMessage(id: string, updates: Partial<Message>) {
  messages.update(msgs =>
    msgs.map(msg => msg.id === id ? { ...msg, ...updates } : msg)
  );
}

export function appendToMessage(id: string, content: string) {
  messages.update(msgs =>
    msgs.map(msg =>
      msg.id === id
        ? { ...msg, content: msg.content + content }
        : msg
    )
  );
}
```

### UI State Store

```typescript
// stores/ui.ts
import { writable } from 'svelte/store';

export const isThinking = writable(false);
export const isStreaming = writable(false);
export const errorMessage = writable<string | null>(null);

// Current message input
export const currentInput = writable('');
```

### Backend State (In-Memory)

```typescript
// backend/src/services/session.ts
interface SessionData {
  sessionId: string;
  messages: Message[];
  lastActivity: number;
}

// In-memory map (for MVP, no persistence)
const sessions = new Map<string, SessionData>();
```

---

## Validation Rules Summary

| Entity | Field | Rule | Error Message |
|--------|-------|------|---------------|
| Message | id | Must be valid UUID v4 | "Invalid message ID format" |
| Message | content (user) | Min 1 char, max 50K chars | "Message cannot be empty" |
| Message | sender | Must be 'user' or 'llm' | "Invalid sender type" |
| Message | timestamp | Must be positive integer | "Invalid timestamp" |
| Message | status | Must be valid MessageStatus | "Invalid message status" |
| StreamChunk | messageId | Must be valid UUID v4 | "Invalid message reference" |
| StreamChunk | type | Must be valid ChunkType | "Invalid chunk type" |

---

## Data Flow Diagrams

### User Message Flow

```
1. User types message in MessageInput
   ↓
2. Frontend validates (non-empty)
   ↓
3. Generate UUID, create Message { status: 'pending', sender: 'user' }
   ↓
4. Add to messages store
   ↓
5. POST /api/chat with message content
   ↓
6. Update message status: 'pending' → 'complete'
```

### LLM Response Flow

```
1. Backend receives POST /api/chat
   ↓
2. Generate UUID for LLM message
   ↓
3. Create Message { status: 'pending', sender: 'llm', content: '' }
   ↓
4. Return message via first SSE event
   ↓
5. Frontend adds message to store, status: 'pending' → 'streaming'
   ↓
6. Backend streams chunks via SSE
   ↓
7. Frontend appends each chunk to message.content
   ↓
8. Backend sends 'done' chunk
   ↓
9. Frontend updates status: 'streaming' → 'complete'
```

---

## Complete TypeScript Types Export

```typescript
// shared/types.ts

// Message types
export type MessageStatus = 'pending' | 'streaming' | 'complete' | 'error';

export interface Message {
  id: string;
  content: string;
  sender: 'user' | 'llm';
  timestamp: number;
  status: MessageStatus;
  reasoning?: ReasoningBlock | null;
}

// Reasoning block
export interface ReasoningBlock {
  content: string;
  isExpanded: boolean;
}

// Session types
export type SessionState = 'idle' | 'thinking' | 'streaming' | 'error';

export interface ConversationSession {
  messages: Message[];
  state: SessionState;
}

// Stream chunk types
export type ChunkType = 'chunk' | 'reasoning' | 'done' | 'error';

export interface StreamChunk {
  type: ChunkType;
  content: string;
  messageId: string;
}

// API request/response types
export interface ChatRequest {
  message: string;
  sessionId?: string;
}

export interface ErrorResponse {
  error: string;
  code: string;
  timestamp: number;
}
```

---

**Data Model Complete** ✅

Next: Generate API contracts (OpenAPI spec)